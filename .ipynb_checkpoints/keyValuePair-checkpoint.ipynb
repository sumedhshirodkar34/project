{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (20.0.2)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\spark\\spark\\python (2.4.3)\n",
      "Requirement already satisfied: py4j==0.10.7 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages (from pyspark) (0.10.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairRDD = sc.parallelize([(1,2),(2,3),(2,4),(3,4),(3,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 7), (3, 9)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduceRDD = pairRDD.reduceByKey(lambda x,y : (x+y))\n",
    "reduceRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x795de90>),\n",
       " (2, <pyspark.resultiterable.ResultIterable at 0x795d9f0>),\n",
       " (3, <pyspark.resultiterable.ResultIterable at 0x795dfb0>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupByRDD = pairRDD.groupByKey()\n",
    "groupByRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check partitions \n",
    "groupByRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [2]\n",
      "2 [3, 4]\n",
      "3 [4, 5]\n"
     ]
    }
   ],
   "source": [
    "#Print out of groupByRDD\n",
    "for t in groupByRDD.collect():\n",
    "    print(t[0],[v for v in t[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 9), (2, 16), (3, 16), (3, 25)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapValuesRDD = pairRDD.mapValues(lambda x : x*x)\n",
    "mapValuesRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 4), (3, 4)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatMapValuesRDD = pairRDD.flatMapValues(lambda x : range(x, 5))\n",
    "flatMapValuesRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 2, 3, 3]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = pairRDD.keys()\n",
    "keys.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 4, 5]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = pairRDD.values()\n",
    "values.collect()\n",
    "# check why two actions can not work in one cell of jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3), (2, 4), (3, 4), (3, 5)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortByKetRDD = pairRDD.sortByKey()\n",
    "sortByKetRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairRDD2 = sc.parallelize([(1, 2),(3, 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 3), (2, 4)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtractByKeyRDD = pairRDD.subtractByKey(pairRDD2)\n",
    "subtractByKeyRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 2)), (3, (4, 5)), (3, (5, 5))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinRDD = pairRDD.join(pairRDD2)\n",
    "joinRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fc44f0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fc46b0>)),\n",
       " (2,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fc43b0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fc4e70>)),\n",
       " (3,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7fc4bf0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7fc4970>))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coGroupRDD = pairRDD.cogroup(pairRDD2)\n",
    "coGroupRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<pyspark.resultiterable.ResultIterable at 0x8075410>,\n",
       "  <pyspark.resultiterable.ResultIterable at 0x8075a90>),\n",
       " (<pyspark.resultiterable.ResultIterable at 0x80753f0>,\n",
       "  <pyspark.resultiterable.ResultIterable at 0x8075430>),\n",
       " (<pyspark.resultiterable.ResultIterable at 0x8075450>,\n",
       "  <pyspark.resultiterable.ResultIterable at 0x8075470>)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coGroupRDD.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x07DB4F70>\n",
      "<generator object <genexpr> at 0x07DB4F70>\n",
      "<generator object <genexpr> at 0x07DB4F70>\n",
      "<generator object <genexpr> at 0x07DB4F70>\n",
      "<generator object <genexpr> at 0x07DB4F70>\n",
      "<generator object <genexpr> at 0x07DB4F70>\n"
     ]
    }
   ],
   "source": [
    "#Print out of coGroupRDD\n",
    "for t in coGroupRDD.collect():\n",
    "    for x in t[1]:\n",
    "        print (y for y in x)\n",
    "#print(t[0],[v for v in t[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 1)), (2, (3, 1)), (2, (4, 1)), (3, (4, 1)), (3, (5, 1))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapvValuesAvgRDD = pairRDD.mapValues(lambda x : (x,1))\n",
    "mapvValuesAvgRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 1)), (2, (7, 2)), (3, (9, 2))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduceByKeyAvgRDD =  mapvValuesAvgRDD.reduceByKey(lambda x,y : (x[0]+y[0], x[1]+y[1]))\n",
    "reduceByKeyAvgRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"README_test.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1),\n",
       " ('Apache', 1),\n",
       " ('Spark', 15),\n",
       " ('', 72),\n",
       " ('is', 7),\n",
       " ('It', 2),\n",
       " ('provides', 1),\n",
       " ('high-level', 1),\n",
       " ('APIs', 1),\n",
       " ('in', 5)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = lines.flatMap(lambda x : x.split(\" \"))\n",
    "result = words.map(lambda x: (x,1)).reduceByKey(lambda x,y : (x+y))\n",
    "result.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countByValue is action \n",
    "wordCount = lines.flatMap(lambda x : x.split(\" \")).countByValue()\n",
    "#wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combineByKey\n",
    "#  (lambda x : (x,1)) will create like (2,(2,1))\n",
    "#  (lambda x, y : (x[0] + y, x[1]+1)) will be like for key 2 is (2,1),3 : (2+3), (1+1)\n",
    "#  (lambda x,y : (x[0]+y[0], x[1]+y[1])) will be like (2,1),(3,1): ((2+3),(1+1))\n",
    "sumCount = pairRDD.combineByKey((lambda x : (x,1)),\n",
    "                                (lambda x, y : (x[0] + y, x[1]+1)),\n",
    "                                (lambda x,y : (x[0]+y[0], x[1]+y[1]))\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 1)), (2, (7, 2)), (3, (9, 2))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumCount.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-49-e265078603d0>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-49-e265078603d0>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    resultSumCountRDD = sumCount.map(lambda(key, (x,y)) : (key , x / y))\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#sumCount.values\n",
    "resultSumCountRDD = sumCount.map(lambda(key, (x,y)) : (key , x / y))\n",
    "#resultSumCountRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[68] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultSumCountRDD.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitionNumber = pairRDD.getNumPartitions()\n",
    "partitionNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#newPartitionNumber = pairRDD.repartition(2)\n",
    "newPartitionNumber = pairRDD.coalesce(3)\n",
    "newPartitionNumber.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patitioning reduce data shuffle over netowrk.\n",
    "# use cache()/persist() on partioned RDD so that same RDD did not get evaluated again again and it saves in memory \n",
    "# if parent RDD is partitioned and we need to maintain partioned then use mapValues() instead of map() and use\n",
    "# flatMapvalues() intead of flatMap() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
